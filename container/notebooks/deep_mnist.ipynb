{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This notebook contains code to train a neural network with a single hidden layer on MNIST. At the end is a short exercise to add a second hidden layer, transforming it into a deep neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import math\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LOGDIR = './graphs' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mnist = input_data.read_data_sets('/tmp/data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_CLASSES = 10\n",
    "NUM_PIXELS = 28 * 28\n",
    "TRAIN_STEPS = 1000\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "HIDDEN1_UNITS = 128\n",
    "\n",
    "LEARNING_RATE = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, NUM_PIXELS], name=\"pixels\")\n",
    "y_ = tf.placeholder(tf.float32, [None, NUM_CLASSES], name=\"labels\")\n",
    "\n",
    "# helped functions to create a weight and bias variable\n",
    "# with proper initialization \n",
    "def weight_variable(inputs, outputs, name):\n",
    "    # why do we initialize weights this way?\n",
    "    # see http://cs231n.github.io/neural-networks-2/ for more details\n",
    "    initial = tf.truncated_normal(shape=[inputs, outputs], stddev=1.0 / math.sqrt(float(inputs)))\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "def bias_variable(shape, name):\n",
    "    initial = tf.constant(0.1, shape=[shape])\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "with tf.name_scope(\"hidden_layer_1\"):\n",
    "\n",
    "    # weights and baises for the first layer\n",
    "    weights1 = weight_variable(NUM_PIXELS, HIDDEN1_UNITS, \"weights1\")\n",
    "    biases1 = bias_variable(HIDDEN1_UNITS, \"biases1\")\n",
    "\n",
    "    # activations for the first hidden layer\n",
    "    hidden1 = tf.nn.relu(tf.matmul(x, weights1) + biases1, name=\"hidden1\")\n",
    "\n",
    "with tf.name_scope(\"output_layer\"):\n",
    "    \n",
    "    # weights and biases for the second layer\n",
    "    weights2 = weight_variable(HIDDEN1_UNITS, NUM_CLASSES, \"weights2\")\n",
    "    biases2 = bias_variable(NUM_CLASSES, \"biases2\")\n",
    "\n",
    "    # logits - you can think of these (roughly)\n",
    "    # as unnormalized probabilities, or the amount of\n",
    "    # evidence we have that the input image corresponds to\n",
    "    # each digit\n",
    "    y = tf.matmul(hidden1, weights2) + biases2\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y, labels=y_))\n",
    "    tf.summary.scalar('loss', loss)\n",
    "\n",
    "with tf.name_scope(\"optimizer\"):\n",
    "    train = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(loss)\n",
    "    \n",
    "\n",
    "train_writer = tf.summary.FileWriter(os.path.join(LOGDIR, \"train\"))\n",
    "train_writer.add_graph(sess.graph)\n",
    "\n",
    "test_writer = tf.summary.FileWriter(os.path.join(LOGDIR, \"test\"))\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "summary_op = tf.summary.merge_all()\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(TRAIN_STEPS):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(BATCH_SIZE)\n",
    "    summary_result, _ = sess.run([summary_op, train], \n",
    "                                    feed_dict={x: batch_xs, y_: batch_ys})\n",
    "\n",
    "    train_writer.add_summary(summary_result, step)\n",
    "    train_writer.add_run_metadata(tf.RunMetadata(), 'step%03d' % step)\n",
    "    \n",
    "    # calculate accuracy on the test set\n",
    "    if step % 100 == 0:\n",
    "        summary_result, acc = sess.run([summary_op, accuracy], \n",
    "                                       feed_dict={x: mnist.test.images, \n",
    "                                                  y_: mnist.test.labels})\n",
    "        test_writer.add_summary(summary_result, step)\n",
    "        test_writer.add_run_metadata(tf.RunMetadata(), 'step%03d' % step)\n",
    "        print (\"test accuracy: %f at step %d\" % (acc, step))\n",
    "\n",
    "\n",
    "print(\"Accuracy %f\" % sess.run(accuracy, \n",
    "                               feed_dict={x: mnist.test.images,\n",
    "                                          y_: mnist.test.labels}))\n",
    "\n",
    "train_writer.close()\n",
    "test_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Exercise\n",
    "\n",
    "Add a second hidden layer to the above code, with 64 units. Experiment with the parameters (batch size, steps, learning rate, units per layer) to see if you can achieve higher accuracy than the single hidden layer model. Keep in mind there's randomness between runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Put your solution here or modify the above code."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
